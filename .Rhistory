"' target='_blank' rel='noopener noreferrer'>", doi, "</a>")
} else ""
)
# Use PubMed pubdate
pubdate_raw <- rec$pubdate
date_parsed <- parse_date_time(pubdate_raw, orders = c("Y b d", "Y b", "Y"))
# Fallback if parsing fails
if (is.na(date_parsed)) {
pub_year <- substr(pubdate_raw, 1, 4)
date_parsed <- ymd(paste0(pub_year, "-01-01"))
} else {
pub_year <- year(date_parsed)  # lubridate::year
}
# Format as YYYY-MM-DD
date <- format(date_parsed, "%Y-%m-%d")
# Filename with counter to avoid overwrites
first_author <- rec$sortfirstauthor
first_author <- gsub("[^A-Za-z0-9]", "", first_author)
base_name <- paste0(first_author, pub_year)
file_name <- file.path(output_dir, paste0(base_name, ".md"))
counter <- 1
while (file.exists(file_name)) {
file_name <- file.path(output_dir, paste0(base_name, "_", counter, ".md"))
counter <- counter + 1
}
# Build front matter
fm <- c(
"---",
paste0('title: "', title, '"'),
paste0('publication: "', pub_str, '"'),
"authors:",
authors_yaml,
paste0('date: "', date, '"'),
# ðŸ”‘ Write pub_keys into YAML if not empty
if (length(pub_keys) > 0) paste0("pub_keys: [", paste(pub_keys, collapse = ", "), "]"),
"---"
)
# Write file
writeLines(fm, file_name, useBytes = TRUE)
message("âœ… Written: ", file_name)
# Set paths
# bib_file <- "~/Dropbox/DataHealthLab/Website/backup/MyLibrary_Aug2025.bib"   # your BibTeX file
output_dir <- "content/publication_all"  # where .md files will go
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)
## pubmed fetch
orcid_id <- "0000-0002-4677-6862"  # <<-- put your ORCID here
articles <- works(orcid_id)
# Extract DOIs
dois <- articles %>% identifiers("doi") %>% tolower() %>% unique()
# ----- SAFE YAML -----
escape_yaml <- function(x) {
if (length(x) == 0 || is.null(x)) return("")
x <- as.character(x)
x <- gsub("\\\\", "\\\\\\\\", x)   # escape backslashes
x <- gsub("\"", "\\\\\"", x)       # escape double quotes
x <- gsub("[{}]", "", x)           # strip braces
trimws(x)
}
# ----- FORMAT AUTHORS FOR PUBMED -----
format_authors_pubmed <- function(df) {
if (nrow(df) == 0) return("[]")
names_vec <- df$name
me_name <- "- otavioranzani"
# Identify all positions of "Ranzani" in names (any variant)
me_idx <- grep("Ranzani", names_vec, ignore.case = TRUE)
n <- length(names_vec)
out <- c()
if (n <= 10) {
# List all authors, replacing your name
for (i in seq_len(n)) {
if (i %in% me_idx) {
out <- c(out, me_name)
} else {
out <- c(out, paste0("- ", names_vec[i]))
}
}
} else {
# >10 authors: first 3, last 2, insert you in middle if not in first3 or last2
first3_idx <- 1:3
last2_idx  <- (n-1):n
# Build first3
first3 <- lapply(names_vec[first3_idx], function(x) {
if (grepl("Ranzani", x, ignore.case = TRUE)) me_name else paste0("- ", x)
})
# Build last2
last2 <- lapply(names_vec[last2_idx], function(x) {
if (grepl("Ranzani", x, ignore.case = TRUE)) me_name else paste0("- ", x)
})
# Middle: only if your name is not in first3 or last2
middle <- c()
if (length(me_idx) > 0) {
me_pos <- me_idx[1]
if (!(me_pos %in% c(first3_idx, last2_idx))) {
middle <- c("- ..", me_name, "- ...")
}
}
# Combine final list
out <- c(first3, middle, last2)
}
unlist(out)
}
# Function to assign pub_keys based on title matches
assign_pub_keys <- function(title, patterns, keys) {
matches <- keys[ vapply(patterns, function(p) grepl(p, title, ignore.case = TRUE), logical(1)) ]
if(length(matches) == 0) matches <- "unknown"   # fallback
unique(matches)
}
# ----- FETCH PUBMED BY DOI -----
fetch_pubmed <- function(doi){
res <- entrez_search(db="pubmed", term=paste0(doi,"[DOI]"))
if(length(res$ids) == 0) return(NULL)
entrez_summary(db="pubmed", id=res$ids[1])
}
# ----- LOOP DOIs -----
for (doi in dois) {
rec <- fetch_pubmed(doi)
if(is.null(rec)) next
# Title
title <- escape_yaml(rec$title)
title <- gsub("\\.$", "", title)
# Define your dictionary
patterns <- c(
"covid|sars[- ]?cov-2",
"post.?infection|long COVID|postacute sequelae",
"tuberculosis|\\btb\\b",
"ICU|critical care|sepsis|ventilator|intensive care|septic|airway|intubation|ketoacidosis|critically ill|cardiac arrest",
"pneumonia|VAP|CAP|respiratory infection|lower respiratory",
"tele|telemedicine|Tele",
"pollution|biomass",
"temperature|heat|cold",
'vaccine|vaccination',
'land-use|landslid|urban|walnut|environment|Household',
'cardiac surgery|mechanical ventilation|ARDS|trachea|acute respiratory distress syndrome|critical illness',
'severe respiratory failure|blood transfusion|life-sustaining treatments|central venous catheter|resistant|resistance|immunocompromised'
)
keys <- c(
"covid",
"post-infection",
"tuberculosis",
"critical care",
"pneumonia",
"telemedicine",
"air pollution",
"temperature",
'vaccine',
'environment',
'critical care',
'critical care'
)
# ðŸ”‘ Assign pub_keys based on the title
pub_keys <- assign_pub_keys(title, patterns, keys)
# Authors
authors_yaml <- format_authors_pubmed(rec$authors)
# Journal info
journal <- escape_yaml(rec$source)
volume  <- if(!is.null(rec$volume)) rec$volume else ""
issue   <- if(!is.null(rec$issue)) rec$issue else ""
pages   <- if(!is.null(rec$pages)) rec$pages else ""
journal_clean <- gsub("\\.$", "", journal)
# Build "publication" string
pub_str <- paste0(
"**", journal_clean, "**",
if(volume!="") paste0(". ", volume) else "",
if(issue!="") paste0("(", issue, ")") else "",
if(pages!="") paste0(":", pages) else "",
if (!is.null(doi) && nzchar(doi)) {
paste0(". <a href='https://doi.org/", doi,
"' target='_blank' rel='noopener noreferrer'>", doi, "</a>")
} else ""
)
# Use PubMed pubdate
pubdate_raw <- rec$pubdate
date_parsed <- parse_date_time(pubdate_raw, orders = c("Y b d", "Y b", "Y"))
# Fallback if parsing fails
if (is.na(date_parsed)) {
pub_year <- substr(pubdate_raw, 1, 4)
date_parsed <- ymd(paste0(pub_year, "-01-01"))
} else {
pub_year <- year(date_parsed)  # lubridate::year
}
# Format as YYYY-MM-DD
date <- format(date_parsed, "%Y-%m-%d")
# Filename with counter to avoid overwrites
first_author <- rec$sortfirstauthor
first_author <- gsub("[^A-Za-z0-9]", "", first_author)
base_name <- paste0(first_author, pub_year)
file_name <- file.path(output_dir, paste0(base_name, ".md"))
counter <- 1
while (file.exists(file_name)) {
file_name <- file.path(output_dir, paste0(base_name, "_", counter, ".md"))
counter <- counter + 1
}
# Build front matter
fm <- c(
"---",
paste0('title: "', title, '"'),
paste0('publication: "', pub_str, '"'),
"authors:",
authors_yaml,
paste0('date: "', date, '"'),
# ðŸ”‘ Write pub_keys into YAML if not empty
if (length(pub_keys) > 0) paste0("pub_keys: [", paste(pub_keys, collapse = ", "), "]"),
"---"
)
# Write file
writeLines(fm, file_name, useBytes = TRUE)
message("âœ… Written: ", file_name)
}
#### GOOGLE SCHOLAR ####
my_id <- "fXwnz6AAAAAJ"
# List all publications
pubs <- get_publications(my_id)
# Inspect
head(pubs)
# Citations per year
cites_year <- get_citation_history(my_id)
plot(cites_year$year, cites_year$cites, type="b", main="Citations per year")
words <- pubs %>%
unnest_tokens(word, title) %>%
count(word, sort = TRUE) %>%
filter(!word %in% stop_words$word)
wordcloud2(words, size = 1)
library(htmlwidgets)
# Generate the wordcloud widget
wc <- wordcloud2(words, size = 1)
# Save as standalone HTML file
saveWidget(wc, "static/media/wordcloud_publications.html", selfcontained = TRUE)
## jones
res <- entrez_search(db="pubmed", term=paste0(25479473,"[PMID]"))
rec <- entrez_summary(db="pubmed", id=res$ids[1])
# Title
title <- escape_yaml(rec$title)
title <- gsub("\\.$", "", title)
# Define your dictionary
patterns <- c(
"covid|sars[- ]?cov-2",
"post.?infection|long COVID|postacute sequelae",
"tuberculosis|\\btb\\b",
"ICU|critical care|sepsis|ventilator|intensive care|septic|airway|intubation|ketoacidosis|critically ill|cardiac arrest",
"pneumonia|VAP|CAP|respiratory infection|lower respiratory",
"tele|telemedicine|Tele",
"pollution|biomass",
"temperature|heat|cold",
'vaccine|vaccination',
'land-use|landslid|urban|walnut|environment|Household',
'cardiac surgery|mechanical ventilation|ARDS|trachea|acute respiratory distress syndrome|critical illness',
'severe respiratory failure|blood transfusion|life-sustaining treatments|central venous catheter|resistant|resistance|immunocompromised'
)
keys <- c(
"covid",
"post-infection",
"tuberculosis",
"critical care",
"pneumonia",
"telemedicine",
"air pollution",
"temperature",
'vaccine',
'environment',
'critical care',
'critical care'
)
# ðŸ”‘ Assign pub_keys based on the title
pub_keys <- assign_pub_keys(title, patterns, keys)
# Authors
authors_yaml <- format_authors_pubmed(rec$authors)
# Journal info
journal <- escape_yaml(rec$source)
volume  <- if(!is.null(rec$volume)) rec$volume else ""
issue   <- if(!is.null(rec$issue)) rec$issue else ""
pages   <- if(!is.null(rec$pages)) rec$pages else ""
journal_clean <- gsub("\\.$", "", journal)
# Build "publication" string
pub_str <- paste0(
"**", journal_clean, "**",
if(volume!="") paste0(". ", volume) else "",
if(issue!="") paste0("(", issue, ")") else "",
if(pages!="") paste0(":", pages) else "",
if (!is.null(doi) && nzchar(doi)) {
paste0(". <a href='https://doi.org/", doi,
"' target='_blank' rel='noopener noreferrer'>", doi, "</a>")
} else ""
)
# Use PubMed pubdate
pubdate_raw <- rec$pubdate
date_parsed <- parse_date_time(pubdate_raw, orders = c("Y b d", "Y b", "Y"))
# Fallback if parsing fails
if (is.na(date_parsed)) {
pub_year <- substr(pubdate_raw, 1, 4)
date_parsed <- ymd(paste0(pub_year, "-01-01"))
} else {
pub_year <- year(date_parsed)  # lubridate::year
}
# Format as YYYY-MM-DD
date <- format(date_parsed, "%Y-%m-%d")
# Filename with counter to avoid overwrites
first_author <- rec$sortfirstauthor
first_author <- gsub("[^A-Za-z0-9]", "", first_author)
base_name <- paste0(first_author, pub_year)
file_name <- file.path(output_dir, paste0(base_name, ".md"))
counter <- 1
while (file.exists(file_name)) {
file_name <- file.path(output_dir, paste0(base_name, "_", counter, ".md"))
counter <- counter + 1
}
# Build front matter
fm <- c(
"---",
paste0('title: "', title, '"'),
paste0('publication: "', pub_str, '"'),
"authors:",
authors_yaml,
paste0('date: "', date, '"'),
# ðŸ”‘ Write pub_keys into YAML if not empty
if (length(pub_keys) > 0) paste0("pub_keys: [", paste(pub_keys, collapse = ", "), "]"),
"---"
)
# Write file
writeLines(fm, file_name, useBytes = TRUE)
message("âœ… Written: ", file_name)
wc
# Render as PNG
png_file <- "content/post/21-11-27-worldcloud-publications/featured.png"
# Render as PNG
png_file <- "wordcloud_publications.png"
webshot2::webshot(url = "static/media/wordcloud_publications.html", file = png_file,
vwidth = 1200, vheight = 1200)  # square size
install.packages("webshot2")
webshot2::webshot(url = html_file, file = png_file,
vwidth = 1200, vheight = 1200)
webshot2::webshot(url = "static/media/wordcloud_publications.html", file = png_file,
vwidth = 1200, vheight = 1200)
library(tidyverse)
library(tidylog)
library(rcrossref)
library(scholar)
library(tidytext)
library(wordcloud2)
library(rentrez)
library(rorcid)
library(openalexR)
library(dimensionsR)
library(htmlwidgets)
library(rscopus)
# ---- STEP 1: Get your DOIs from ORCID ----
orcid_id <- "0000-0002-4677-6862"  # <<-- put your ORCID here
articles <- works(orcid_id)
# Extract DOIs
dois <- articles %>% identifiers("doi") %>% tolower() %>% unique()
# ---- STEP 2: For each DOI, get citing works ----
get_citing_dois <- function(doi) {
citing <- try(cr_cn(doi, works = TRUE), silent = TRUE)
if (inherits(citing, "try-error") || is.null(citing$data)) return(NULL)
citing$data
}
# ---- STEP 3: Check if citing paper is a guideline ----
is_guideline <- function(doi) {
res <- try(entrez_search(db="pubmed", term=paste0(doi, "[DOI]"), retmax=1), silent = TRUE)
if (inherits(res, "try-error") || length(res$ids) == 0) return(FALSE)
summary <- entrez_summary(db="pubmed", id=res$ids)
pt <- summary$pubtype
any(grepl("Guideline|Consensus|Recommendation|Practice Guideline|Position Statement",
pt, ignore.case = TRUE))
}
# ---- STEP 4: Loop through your publications ----
results <- list()
for (my_doi in pubs) {
message("Checking: ", my_doi)
citing_df <- get_citing_dois(my_doi)
if (!is.null(citing_df) && nrow(citing_df) > 0) {
citing_df <- citing_df %>%
mutate(is_guideline = map_lgl(doi, is_guideline),
cited_my_doi = my_doi) %>%
filter(is_guideline)
if (nrow(citing_df) > 0) {
results[[length(results)+1]] <- citing_df
}
}
}
dimensions <- map(dois, function(d) {
res <- tryCatch(altmetric(d), error = function(e) NULL)
# Skip nulls or non-results
if (is.null(res)) return(NULL)
# Coerce to tibble safely (convert list â†’ data.frame)
res_df <- as.data.frame(res, stringsAsFactors = FALSE)
if (nrow(res_df) > 0) {
return(res_df)
} else {
return(NULL)
}
})
dimensions <- bind_rows(dimensions)
dimensions %>% count(cited_by_patents_count)
dimensions %>% filter(!is.na(cited_by_patents_count)) %>% select(title) %>% count()
dimensions %>% filter(!is.na(cited_by_patents_count)) %>% select(title)
dimensions %>% count(cited_by_policies_count)
dimensions %>% filter(!is.na(cited_by_policies_count)) %>% select(title)
dimensions %>% count(cited_by_guidelines_count)
dimensions %>% filter(!is.na(cited_by_guidelines_count)) %>% select(title) %>% count()
dimensions %>% count(cited_by_guidelines_count) %>% filter(!is.na(cited_by_guidelines_count)) %>% mutate(tot = as.numeric(cited_by_guidelines_count)*n, tot = cumsum(tot))
dimensions %>% filter(!is.na(cited_by_guidelines_count)) %>% select(title)
dimensions %>% count(cited_by_wikipedia_count)
dimensions %>% filter(!is.na(cited_by_wikipedia_count)) %>% select(title) %>% count()
dimensions %>% count(cited_by_wikipedia_count) %>% filter(!is.na(cited_by_wikipedia_count)) %>% mutate(tot = as.numeric(cited_by_wikipedia_count)*n, tot = cumsum(tot))
View(articles)
articles %>% count(type)
citations <- arrow::read_parquet('~/Dropbox/DataHealthLab/Website/backup/citations_280825.parquet')
citations %>% filter(str_detect(title_clean, 'guidel|societ|consens|joint|statement')) %>% View
library(httr)
library(jsonlite)
library(rcrossref)
library(dplyr)
library(purrr)
library(stringr)
library(stringi)
citations %>% filter(str_detect(title_clean, 'guidel|societ|consens|joint|statement')) %>% View
citations %>% filter(str_detect(title_clean, 'recu'))
citations %>% filter(str_detect(title_clean, 'recuvap'))
citations %>% filter(str_detect(title_clean, 'recuvap')) %>% pull(title)
citations %>% filter(str_detect(title_clean, 'recuvap')) %>% pull(title_clean)
citations %>% filter(str_detect(title_clean, 'recuvap')) %>% pull(title_clean) %>% unique
res <- cr_works(query = "a consensus of european experts on the definition of ventilator-associated pneumonia recurrences obtained by the delphi method: the recuvap study")
res$data$DOI[1]
View(res)
res[["data"]][["doi"]]
res[["data"]][["alternative.id"]]
res[["data"]][["doi"]]
res[["data"]][["url"]]
View(citations)
citations %>% filter(str_detect(title_clean, 'recuvap')) %>% pull(title_clean) %>% unique
citations %>% filter(str_detect(title_clean, 'guidel|societ|consens|joint|statement')) %>% View
View(citations)
citations %>%
mutate(
title_clean = title %>%
stri_trans_general("Latin-ASCII") %>%        # Normalize accents
str_replace_all("[[:punct:]]", " ") %>%      # Remove all punctuation
str_replace_all("\\s+", " ") %>%             # Replace multiple spaces
str_to_lower() %>%                           # To lowercase
str_trim()                                   # Trim leading/trailing spaces
) %>%
distinct(original_doi, title_clean, .keep_all=T)
library(tidylog)
citations %>%
mutate(
title_clean = title %>%
stri_trans_general("Latin-ASCII") %>%        # Normalize accents
str_replace_all("[[:punct:]]", " ") %>%      # Remove all punctuation
str_replace_all("\\s+", " ") %>%             # Replace multiple spaces
str_to_lower() %>%                           # To lowercase
str_trim()                                   # Trim leading/trailing spaces
) %>%
distinct(original_doi, title_clean, .keep_all=T)
citations %>%
mutate(
title_clean = title %>%
stri_trans_general("Latin-ASCII") %>%        # Normalize accents
str_replace_all("[[:punct:]]", " ") %>%      # Remove all punctuation
str_replace_all("\\s+", " ") %>%             # Replace multiple spaces
str_to_lower() %>%                           # To lowercase
str_trim()                                   # Trim leading/trailing spaces
) %>%
distinct(original_doi, title_clean, .keep_all=T) %>%
filter(str_detect(title_clean, 'guidel|societ|consens|joint|statement')) %>% View
citations %>%
mutate(
title_clean = title %>%
stri_trans_general("Latin-ASCII") %>%        # Normalize accents
str_replace_all("[[:punct:]]", " ") %>%      # Remove all punctuation
str_replace_all("\\s+", " ") %>%             # Replace multiple spaces
str_to_lower() %>%                           # To lowercase
str_trim()                                   # Trim leading/trailing spaces
) %>%
distinct(original_doi) %>% count()
citations %>%
mutate(
title_clean = title %>%
stri_trans_general("Latin-ASCII") %>%        # Normalize accents
str_replace_all("[[:punct:]]", " ") %>%      # Remove all punctuation
str_replace_all("\\s+", " ") %>%             # Replace multiple spaces
str_to_lower() %>%                           # To lowercase
str_trim()                                   # Trim leading/trailing spaces
) %>%
distinct(original_doi, title_clean, .keep_all=T) %>%
filter(str_detect(title_clean, 'guidel|societ|consens|joint|statement')) %>%
count() # 178
citations %>%
mutate(
title_clean = title %>%
stri_trans_general("Latin-ASCII") %>%        # Normalize accents
str_replace_all("[[:punct:]]", " ") %>%      # Remove all punctuation
str_replace_all("\\s+", " ") %>%             # Replace multiple spaces
str_to_lower() %>%                           # To lowercase
str_trim()                                   # Trim leading/trailing spaces
) %>%
distinct(original_doi, title_clean, .keep_all=T) %>%
filter(str_detect(title_clean, 'guidel|societ|consens|joint|statement')) %>%
distinct(original_doi) %>%
count() # 139
citations %>%
mutate(
title_clean = title %>%
stri_trans_general("Latin-ASCII") %>%        # Normalize accents
str_replace_all("[[:punct:]]", " ") %>%      # Remove all punctuation
str_replace_all("\\s+", " ") %>%             # Replace multiple spaces
str_to_lower() %>%                           # To lowercase
str_trim()                                   # Trim leading/trailing spaces
) %>%
distinct(original_doi, title_clean, .keep_all=T) %>%
filter(str_detect(title_clean, 'guidel|societ|consens|joint|statement')) %>%
distinct(title_clean) %>%
count() # 58
